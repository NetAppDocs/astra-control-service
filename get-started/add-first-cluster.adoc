---
sidebar: sidebar
permalink: get-started/add-first-cluster.html
keywords: discover cluster, add cluster, add kubernetes cluster, discover kubernetes cluster, add cluster
summary: After you set up your environment, you're ready to create a Kubernetes cluster and then add it to Astra Control Service.
---

= Start managing Kubernetes clusters from Astra Control Service
:hardbreaks:
:icons: font
:imagesdir: ../media/get-started/

[.lead]
After you set up your environment, you're ready to create a Kubernetes cluster and then add it to Astra Control Service.

* <<Create a Kubernetes cluster>>
* <<Start managing Kubernetes clusters>>
* <<Change the default storage class>>

== Create a Kubernetes cluster

ifndef::azure,gcp[]
If you don't have a cluster yet, you can create one that meets link:set-up-amazon-web-services.html#eks-cluster-requirements[Astra Control Service requirements for Amazon Elastic Kubernetes Service (EKS)].
endif::azure,gcp[]
ifndef::azure,aws[]
If you don't have a cluster yet, you can create one that meets link:set-up-google-cloud.html#gke-cluster-requirements[Astra Control Service requirements for Google Kubernetes Engine (GKE)].
endif::azure,aws[]
ifndef::gcp,aws[]
If you don't have a cluster yet, you can create one that meets link:set-up-microsoft-azure-with-anf.html#azure-kubernetes-service-cluster-requirements[Astra Control Service requirements for Azure Kubernetes Service (AKS) with Azure NetApp Files] or link:set-up-microsoft-azure-with-amd.html#azure-kubernetes-service-cluster-requirements[Astra Control Service requirements for Azure Kubernetes Service (AKS) with Azure managed disks].

NOTE: Astra Control Service supports AKS clusters that use Azure Active Directory (Azure AD) for authentication and identity management. When you create the cluster, follow the instructions in the https://docs.microsoft.com/en-us/azure/aks/managed-aad[official documentation^] to configure the cluster to use Azure AD. You'll need to make sure your clusters meet the requirements for AKS-managed Azure AD integration.
endif::gcp,aws[]

ifdef::gcp+azure+aws[]
If you don't have a cluster yet, you can create one that meets the requirements of one of the following providers:

* link:set-up-microsoft-azure-with-anf.html[Astra Control Service requirements for Azure Kubernetes Service (AKS) with Azure NetApp Files]
* link:set-up-microsoft-azure-with-amd.html[Astra Control Service requirements for Azure Kubernetes Service (AKS) with Azure managed disks]
* link:set-up-google-cloud.html#gke-cluster-requirements[Astra Control Service requirements for Google Kubernetes Engine (GKE)]
* link:set-up-amazon-web-services.html#eks-cluster-requirements[Astra Control Service requirements for Amazon Elastic Kubernetes Service (EKS)]

NOTE: Astra Control Service supports AKS clusters that use Azure Active Directory (Azure AD) for authentication and identity management. When you create the cluster, follow the instructions in the https://docs.microsoft.com/en-us/azure/aks/managed-aad[official documentation^] to configure the cluster to use Azure AD. You'll need to make sure your clusters meet the requirements for AKS-managed Azure AD integration.

endif::gcp+azure+aws[]

You can add a self-managed cluster to Astra Control Service by uploading a `kubeconfig.yaml` file. You'll need to ensure the cluster meets the requirements outlined in <<Start managing Kubernetes clusters>>.

== Start managing Kubernetes clusters

After you log in to Astra Control Service, your first step is to start managing your clusters. You can add a cluster that is managed by a cloud provider, or a self-managed cluster. Before you add a cluster to Astra Control Service, you'll need to perform certain tasks and make sure the cluster meets certain requirements.

.What you'll need for clusters that are managed by a cloud provider
[%collapsible]
=======

ifdef::aws[]
.Amazon Web Services
* You should have the JSON file containing the credentials of the IAM user that created the cluster. link:../get-started/set-up-amazon-web-services.html#create-an-iam-user[Learn how to create an IAM user].
* Astra Trident is required for Amazon FSx for NetApp ONTAP. If you plan to use Amazon FSx for NetApp ONTAP as a storage backend for your EKS cluster, refer to the Astra Trident information in the link:set-up-amazon-web-services.html#eks-cluster-requirements[EKS cluster requirements].
* (Optional) If you need to provide provide `kubectl` command access for a cluster to other IAM users that are not the cluster's creator, refer to the instructions in https://aws.amazon.com/premiumsupport/knowledge-center/amazon-eks-cluster-access/[How do I provide access to other IAM users and roles after cluster creation in Amazon EKS?^].

endif::aws[]

ifdef::azure[]
.Microsoft Azure
* You should have the JSON file that contains the output from the Azure CLI when you created the service principal. link:../get-started/set-up-microsoft-azure-with-anf.html#create-an-azure-service-principal-2[Learn how to set up a service principal].
+
You'll also need your Azure subscription ID, if you didn't add it to the JSON file.

* For private AKS clusters, refer to link:manage-private-cluster.html[Manage private clusters from Astra Control Service^].
* If you plan to use NetApp Cloud Volumes ONTAP as a storage backend, you need to configure Cloud Volumes ONTAP to work with Microsoft Azure. Refer to the Cloud Volumes ONTAP https://docs.netapp.com/us-en/cloud-manager-cloud-volumes-ontap/task-getting-started-azure.html[setup documentation^].
endif::azure[]

ifdef::gcp[]
.Google Cloud
* You should have the service account key file for a service account that has the required permissions. link:../get-started/set-up-google-cloud.html#create-a-service-account[Learn how to set up a service account].
* If you plan to use NetApp Cloud Volumes ONTAP as a storage backend, you need to configure Cloud Volumes ONTAP to work with Google Cloud. Refer to the Cloud Volumes ONTAP https://docs.netapp.com/us-en/cloud-manager-cloud-volumes-ontap/task-getting-started-gcp.html[setup documentation^].
endif::gcp[]
=======


.What you'll need for self-managed clusters
[%collapsible]
=======
Your self-managed clusters can use Astra Trident to interface with NetApp storage services, or they can use Container Storage Interface (CSI) drivers to interface with other storage services. 

Astra Control Service supports self-managed clusters using the following Kubernetes distributions:

* Red Hat OpenShift Container Platform
* Rancher Kubernetes Engine
* Upstream Kubernetes 

Your self-managed cluster needs to meet the following requirements:

* The cluster must be accessible via the internet.
* If you are using or plan to use storage enabled with CSI drivers, the appropriate CSI drivers must be installed on the cluster. For more information on using CSI drivers to integrate storage, refer to the documentation for your storage service.
*	If you are using or plan to use NetApp storage, ensure that you have installed the latest version of Astra Trident:
+
NOTE: You can https://docs.netapp.com/us-en/trident/trident-get-started/kubernetes-deploy.html#choose-the-deployment-method[deploy Astra Trident^] using either Trident operator (manually or using Helm chart) or `tridentctl`. Prior to installing or upgrading Astra Trident, review the https://docs.netapp.com/us-en/trident/trident-get-started/requirements.html[supported frontends, backends, and host configurations^].

** *Trident storage backend configured*: At least one Astra Trident storage backend must be https://docs.netapp.com/us-en/trident/trident-get-started/kubernetes-postdeployment.html#step-1-create-a-backend[configured^] on the cluster.
** *Trident storage classes configured*: At least one Astra Trident storage class must be https://docs.netapp.com/us-en/trident/trident-use/manage-stor-class.html[configured^] on the cluster. If a default storage class is configured, ensure that only one storage class has that annotation.
** *Astra Trident volume snapshot controller and volume snapshot class installed and configured*: The volume snapshot controller must be https://docs.netapp.com/us-en/trident/trident-use/vol-snapshots.html#deploying-a-volume-snapshot-controller[installed^] so that snapshots can be created in Astra Control. At least one Astra Trident `VolumeSnapshotClass` has been https://docs.netapp.com/us-en/trident/trident-use/vol-snapshots.html#step-1-set-up-a-volumesnapshotclass[set up^] by an administrator.
* *Kubeconfig accessible*: You have access to the <<kubeconfig, cluster kubeconfig>> that includes only one context element.
// Removed ONTAP credentials commands from ACC as Vijitha said they are not needed - ASTRADOC-21
* *Rancher only*: When managing application clusters in a Rancher environment, modify the application cluster's default context in the kubeconfig file provided by Rancher to use a control plane context instead of the Rancher API server context. This reduces load on the Rancher API server and improves performance.


.(Optional) Check Astra Trident version
If your cluster uses Astra Trident for storage services, ensure that the installed version of Astra Trident is the latest. 

.Steps

. Check the Astra Trident version.
+
[source,console]
----
kubectl get tridentversions -n trident
----
+
If Astra Trident is installed, you see output similar to the following:
+
----
NAME      VERSION
trident   22.10.0
----
+
If Astra Trident is not installed, you see output similar to the following:
+
----
error: the server doesn't have a resource type "tridentversions"
----
+
NOTE: If Astra Trident is not installed or not current, and you want your cluster to use Astra Trident for storage services, you need to install the latest version of Astra Trident before proceeding. Refer to the https://docs.netapp.com/us-en/trident/trident-get-started/kubernetes-deploy.html[Astra Trident documentation^] for instructions.

. Ensure that the pods are running:
+
[source,console]
----
kubectl get pods -n trident
----

. Check if the storage classes are using the supported Astra Trident drivers. The provisioner name should be `csi.trident.netapp.io`. Refer to the following example:
+
[source,console]
----
kubectl get sc
----
+
Sample response:
+
----
NAME                   PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
ontap-gold (default)   csi.trident.netapp.io          Delete          Immediate           true                   5d23h
----

[id=kubeconfig]
.Create an admin-role kubeconfig (applies to clusters running Rancher, Openshift, and Upstream Kubernetes)

Ensure that you have the following on your machine before you do the steps:

* kubectl v1.19 or later installed
* An active kubeconfig with cluster admin rights for the active context

.Steps
. Create a service account as follows:
.. Create a service account file called `astracontrol-service-account.yaml`.
+
Adjust the name and namespace as needed. If changes are made here, you should apply the same changes in the following steps.
+
[source]
[subs="specialcharacters,quotes"]
----
*astracontrol-service-account.yaml*
----
+
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: astracontrol-service-account
  namespace: default
----
.. Apply the service account:
+
[source,console]
----
kubectl apply -f astracontrol-service-account.yaml
----
//. (Optional) If your cluster uses a restrictive pod security policy that doesn't allow privileged pod creation or allow processes within the pod containers to run as the root user, create a custom pod security policy for the cluster that enables Astra Control to create and manage pods. For instructions, see link:acc-create-podsecuritypolicy.html[Create a custom pod security policy].
. Grant cluster admin permissions as follows:
.. Create a `ClusterRoleBinding` file called `astracontrol-clusterrolebinding.yaml`.
+
Adjust any names and namespaces modified when creating the service account as needed.
+
[source]
[subs="specialcharacters,quotes"]
----
*astracontrol-clusterrolebinding.yaml*
----
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: astracontrol-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: astracontrol-service-account
  namespace: default
----
.. Apply the cluster role binding:
+
[source,console]
----
kubectl apply -f astracontrol-clusterrolebinding.yaml
----
. List the service account secrets, replacing `<context>` with the correct context for your installation:
+
[source,console]
----
kubectl get serviceaccount astracontrol-service-account --context <context> --namespace default -o json
----
+
The end of the output should look similar to the following:
+
----
"secrets": [
{ "name": "astracontrol-service-account-dockercfg-vhz87"},
{ "name": "astracontrol-service-account-token-r59kr"}
]
----
+
The indices for each element in the `secrets` array begin with 0. In the above example, the index for `astracontrol-service-account-dockercfg-vhz87` would be 0 and the index for `astracontrol-service-account-token-r59kr` would be 1. In your output, make note of the index for the service account name that has the word "token" in it.
. Generate the kubeconfig as follows:
.. Create a `create-kubeconfig.sh` file. Replace `TOKEN_INDEX` in the beginning of the following script with the correct value.
+
[source]
[subs="specialcharacters,quotes"]
----
*create-kubeconfig.sh*
----
+
[source,console]
----
# Update these to match your environment.
# Replace TOKEN_INDEX with the correct value
# from the output in the previous step. If you
# didn't change anything else above, don't change
# anything else here.

SERVICE_ACCOUNT_NAME=astracontrol-service-account
NAMESPACE=default
NEW_CONTEXT=astracontrol
KUBECONFIG_FILE='kubeconfig-sa'

CONTEXT=$(kubectl config current-context)

SECRET_NAME=$(kubectl get serviceaccount ${SERVICE_ACCOUNT_NAME} \
  --context ${CONTEXT} \
  --namespace ${NAMESPACE} \
  -o jsonpath='{.secrets[TOKEN_INDEX].name}')
TOKEN_DATA=$(kubectl get secret ${SECRET_NAME} \
  --context ${CONTEXT} \
  --namespace ${NAMESPACE} \
  -o jsonpath='{.data.token}')

TOKEN=$(echo ${TOKEN_DATA} | base64 -d)

# Create dedicated kubeconfig
# Create a full copy
kubectl config view --raw > ${KUBECONFIG_FILE}.full.tmp

# Switch working context to correct context
kubectl --kubeconfig ${KUBECONFIG_FILE}.full.tmp config use-context ${CONTEXT}

# Minify
kubectl --kubeconfig ${KUBECONFIG_FILE}.full.tmp \
  config view --flatten --minify > ${KUBECONFIG_FILE}.tmp

# Rename context
kubectl config --kubeconfig ${KUBECONFIG_FILE}.tmp \
  rename-context ${CONTEXT} ${NEW_CONTEXT}

# Create token user
kubectl config --kubeconfig ${KUBECONFIG_FILE}.tmp \
  set-credentials ${CONTEXT}-${NAMESPACE}-token-user \
  --token ${TOKEN}

# Set context to use token user
kubectl config --kubeconfig ${KUBECONFIG_FILE}.tmp \
  set-context ${NEW_CONTEXT} --user ${CONTEXT}-${NAMESPACE}-token-user

# Set context to correct namespace
kubectl config --kubeconfig ${KUBECONFIG_FILE}.tmp \
  set-context ${NEW_CONTEXT} --namespace ${NAMESPACE}

# Flatten/minify kubeconfig
kubectl config --kubeconfig ${KUBECONFIG_FILE}.tmp \
  view --flatten --minify > ${KUBECONFIG_FILE}

# Remove tmp
rm ${KUBECONFIG_FILE}.full.tmp
rm ${KUBECONFIG_FILE}.tmp
----
.. Source the commands to apply them to your Kubernetes cluster.
+
[source,console]
----
source create-kubeconfig.sh
----
. (Optional) Rename the kubeconfig to a meaningful name for your cluster. Protect your cluster credential.
+
----
chmod 700 create-kubeconfig.sh
mv kubeconfig-sa YOUR_CLUSTER_NAME_kubeconfig
----
=======

.Steps

. On the Dashboard, select *Manage Kubernetes cluster*.
+
Follow the prompts to add the cluster.

. *Provider*: Select your cloud provider and then either provide the required credentials to create a new cloud instance, or select an existing cloud instance to use.
ifdef::aws[]
.. *Amazon Web Services*: Provide details about your Amazon Web Services IAM user account by uploading a JSON file or by pasting the contents of that JSON file from your clipboard.
+
The JSON file should contain the credentials of the IAM user that created the cluster.
endif::aws[]
ifdef::azure[]
.. *Microsoft Azure*: Provide details about your Azure service principal by uploading a JSON file or by pasting the contents of that JSON file from your clipboard.
+
The JSON file should contain the output from the Azure CLI when you created the service principal. It can also include your subscription ID so it's automatically added to Astra. Otherwise, you need to manually enter the ID after providing the JSON.
endif::azure[]
ifdef::gcp[]
.. *Google Cloud Platform*: Provide the service account key file either by uploading the file or by pasting the contents from your clipboard.
+
Astra Control Service uses the service account to discover clusters running in Google Kubernetes Engine.
endif::gcp[]
.. *Other*: Provide details about your self-managed cluster by uploading a `kubeconfig.yaml` file or by pasting the contents of the `kubeconfig.yaml` file from your clipboard.
+
NOTE: If you create your own `kubeconfig` file, you should define only *one* context element in it. Refer to https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[Kubernetes documentation^] for information about creating `kubeconfig` files.

. *Cloud instance name* (For provider-managed clusters): Provide a name for the new cloud instance that will be created when you add this cluster. Learn more about link:../use/manage-cloud-instances.html[cloud instances].
+

NOTE: When you are selecting from the list of clusters, pay careful attention to the Eligible tab. If a warning appears, hover over the warning to determine if there's an issue with the cluster. For example, it might identify that the cluster doesn't have a worker node. 

ifdef::azure[]
+

NOTE: If you select a cluster that is marked with a "Private" icon, it uses private IP addresses, and the Astra Connector is needed for Astra Control to manage the cluster. If you see a message stating that you need to install the Astra Connector, link:manage-private-cluster.html[refer to these instructions] to install the Astra Connector and enable management of the cluster. After you've installed the Astra Connector, the cluster should be eligible and you can proceed with adding the cluster.
endif::azure[]

. *Credential name* (For self-managed clusters): Provide a name for the self-managed cluster credential you are uploading to Astra Control. By default, the credential name is auto-populated as the name of the cluster.

. (Optional) *Storage*: Select the storage class that you'd like Kubernetes applications deployed to this cluster to use by default.
+

[NOTE]
====
Each cloud provider storage service displays the following price, performance, and resilience information:

ifdef::gcp[]
* Cloud Volumes Service for Google Cloud: Price, performance, and resilience information
* Google Persistent Disk: No price, performance, or resilience information available
endif::gcp[]
ifdef::azure[]
* Azure NetApp Files: Performance and resilience information
* Azure Managed disks: No price, performance, or resilience information available
endif::azure[]
ifdef::aws[]
* Amazon Elastic Block Store: No price, performance, or resilience information available
* Amazon FSx for NetApp ONTAP: No price, performance, or resilience information available
endif::aws[]
* NetApp Cloud Volumes ONTAP: No price, performance, or resilience information available
====
+
Each storage class can utilize one of the following services:

ifdef::gcp[]
* https://cloud.netapp.com/cloud-volumes-service-for-gcp[Cloud Volumes Service for Google Cloud^]
* https://cloud.google.com/persistent-disk/[Google Persistent Disk^]
endif::gcp[]
ifdef::azure[]
* https://cloud.netapp.com/azure-netapp-files[Azure NetApp Files^]
* https://docs.microsoft.com/en-us/azure/virtual-machines/managed-disks-overview[Azure managed disks^]
endif::azure[]
ifdef::aws[]
* https://docs.aws.amazon.com/ebs/[Amazon Elastic Block Store^]
* https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html[Amazon FSx for NetApp ONTAP^]
endif::aws[]
* https://www.netapp.com/cloud-services/cloud-volumes-ontap/what-is-cloud-volumes/[NetApp Cloud Volumes ONTAP^]
+
ifndef::gcp,azure[]
Learn more about link:../learn/aws-storage.html[storage classes for Amazon Web Services clusters].
endif::gcp,azure[]
ifndef::gcp,aws[]
Learn more about link:../learn/azure-storage.html[storage classes for AKS clusters].
endif::gcp,aws[]
ifndef::azure,aws[]
Learn more about link:../learn/choose-class-and-size.html[storage classes for GKE clusters].
endif::azure,aws[]
ifdef::gcp+azure+aws[]
Learn more about link:../learn/aws-storage.html[storage classes for Amazon Web Services clusters], link:../learn/choose-class-and-size.html[storage classes for GKE clusters], and link:../learn/azure-storage.html[storage classes for AKS clusters].
endif::gcp+azure+aws[]
//Each storage class utilizes https://cloud.netapp.com/cloud-volumes-service-for-gcp[Cloud Volumes Service for Google Cloud^] or https://cloud.netapp.com/azure-netapp-files[Azure NetApp Files^].
//+
//* link:../learn/choose-class-and-size.html[Learn about storage classes for GKE clusters].
//* link:../learn/azure-storage.html[Learn about storage classes for AKS clusters].

. *Review & Approve*: Review the configuration details and select *Add cluster*.
//+
//image:screenshot-compute-approve.gif["A screenshot that shows the Review & Approve page, which provides a summary of the configuration that you chose for the managed app."]

//The following video shows each of these steps for a GKE cluster.

//video::video-manage-cluster.mp4[width=848, height=480]

.Result

*For provider-managed clusters*:
If this is the first cluster that you have added for this cloud provider, Astra Control Service creates an object store for the cloud provider for backups of applications running on eligible clusters. (When you add subsequent clusters for this cloud provider, no further object stores are created.) If you specified a default storage class, Astra Control Service sets the default storage class that you specified. For clusters managed in Amazon Web Services or Google Cloud Platform, Astra Control Service also creates an admin account on the cluster. These actions can take several minutes.

//*For self-managed clusters*:
//Astra Control Service creates an admin account on the cluster. This process can take several minutes.

////
.Steps

. Go to *Clusters*.
. Select *Add*.
. Select the *Other* tab.
. In the *Credentials* area, upload a `kubeconfig.yaml` file or paste the contents of a `kubeconfig.yaml` file.
+
NOTE: The `kubeconfig.yaml` file should include *only the cluster credential for one cluster*.

+
NOTE: If you create your own `kubeconfig` file, you should define only *one* context element in it. Refer to https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[Kubernetes documentation^] for information about creating `kubeconfig` files. 

. Provide a credential name. By default, the credential name is auto-populated as the name of the cluster.
. Select *Next*.
. Select the storage class to be used for this Kubernetes cluster, and select *Next*.
. Review the information, and if everything looks good, select *Add*.

////



== Change the default storage class
You can change the default storage class for a cluster.

=== Change the default storage class using Astra Control
You can change the default storage class for a cluster from within Astra Control. If your cluster uses a previously installed storage backend service, you might not be able to use this method to change the default storage class (the *Set as default* action is not selectable). In this case, you can <<Change the default storage class using the command line>>.

.Steps

. In the Astra Control Service UI, select *Clusters*.
. On the *Clusters* page, select the cluster that you want to change.
. Select the *Storage* tab.
. Select the *Storage classes* category.
. Select the *Actions* menu for the storage class that you want to set as default.
. Select *Set as default*.

=== Change the default storage class using the command line
You can change the default storage class for a cluster using Kubernetes commands. This method works regardless of your cluster's configuration.

.Steps

. Log in to your Kubernetes cluster. 
. List the storage classes in your cluster:
+
[source,console]
----
kubectl get storageclass
----
. Remove the default designation from the default storage class. Replace <SC_NAME> with the name of the storage class: 
+
[source,console]
----
kubectl patch storageclass <SC_NAME> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
----
. Mark a different storage class as default. Replace <SC_NAME> with the name of the storage class:
+
[source,console]
----
kubectl patch storageclass <SC_NAME> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
----
. Confirm the new default storage class:
+
[source,console]
----
kubectl get storageclass
----


ifdef::azure[]
== For more information

* link:manage-private-cluster.html[Manage a private cluster]
endif::azure[]